import Approach from '../../api/approach/approach.model';

let p1a1 = '5861484d2c3e861ba6b390f1',
    p1a2 = '5861484d2c3e861ba6b390f2',
    p1a3 = '5861484d2c3e861ba6b390f3',
    p1a4 = '5861484d2c3e861ba6b390f4';

let p2a1 = '5861484d2c3e861ba6b390f5',
    p2a3 = '5861484d2c3e861ba6b390f7',
    p2a4 = '5861484d2c3e861ba6b390f8';

let category1 = '58614771d3a6681abe39abb2',
    category2 = '58614771d3a6681abe39abb3';

let machine1 = '5861484d2c3e861ba6b380f4',
    machine2 = '5861484d2c3e861ba6b380f5',
    machine3 = '5861484d2c3e861ba6b380f6',
    machine4 = '5861484d2c3e861ba6b380f7';

let problem1 = '5861484d2c3e861ba6b380f2',
    problem2 = '5861484d2c3e861ba6b380f3';

let user1 = '58614771d3a6681abe39abb1';

// Create approaches
Approach.find({}).remove()
    .then(() => {
        Approach.create({
            approach_name: "Reduction Using Segment Trees",
            user_id: [user1],
            descP: "We have implemented a segment tree to compute the sum of all the numbers in the array. Each node except the leaf contains sum of a continuous range of array which is used to compute sum of its parent. We go down to the level where there are enough number of disjoint nodes for all threads to work on. Each thread will recursively compute the sum of the contiguous array which that represents.",
            scr_a: "As problem size increases, speed up keeps increasing to an optimal point beyond which its increase is not that significant. With increase in number of processors we see increase in speed up in accordance to gustafson's law. We have a lot of memory access and recursive overhead which does not allow the speed up to go beyond 1. Also the number of operations per iteration has increased by a factor of 10;",
            descS: "Directly add all the numbers in the array using the for loop",
            theo_speed: "n/log(n) (For n processors)",
            granul_a: "Our computation is quite less than number of memory access. Hence our code is coarsely granular.",
            user_id_1: "201401443",
            advantage: "Advantages:\nSegment tree is a more powerful data structure and hence if we update some entries in array, we don't need to traverse the entire array again.\nOur problem is disjoint in nature and we can increase number of processors to cope up.\n\nDisadvantages:\nOur problem size and number of processors must be power of 2 for better results.\nWe are doing 10 times more work per iteration as compared to the serial code and so we are bound to get lesser speed up.\nOur memory access is for i, 2*i, 2*i+1 for each iteration. Hence we can see that cache performance of our code will be inversely affected.\nOur code has a lot of recursive calls which adds recursive overhead.",
            tcr_a_problem: "As problem size increases, the number of computation and number of memory access increase. Hence we see that there is increase in time taken with increase in problem size.",
            complexityS: "O(n)",
            complexityP: "O(n/p)",
            effcr_a: "Efficiency will decrease with increase in number of processors. However, we can see some unusual trends because our problem is best suited for number of processors which are in power of 2. Hence this trend is not followed by 3 processors",
            kfmc_a: "Karpp-Flatt metric is decreasing which mean our serial part is increasing.",
            msapo: "Major overhead is declaring arrays and file descriptors. Apart from that, major overhead in parallel code is when threads are launched and when they are waiting to be synchronized. This is more when number of processors are not power of 2.",
            sched_rel_a: "We would like to assign work to the thread which is free immediately, as this would help reduce the overhead in the case when the number of processors is not power of 2.",
            _id: p1a2,
            costP: "p*(log(n))",
            other_a: "Our Problem size is ideally power of 2 and our processors should also be power of 2 for the best results. However, if not, our code does not provide satisfiable results.",
            user_id_2: "201401456",
            tub: "3.66",
            problem_id: problem1,
            scalab_a: "Our problem is highly scalable as we can always go a level below where the number of disjoint nodes are greater than number of processors.",
            estimated_ser_frac: "0.03",
            false_sharing_a: "As we can see that we are recursively calling both left and right child, and we update our value. However, this may be in cache of other threads as well. So we will update that although we don't need to.",
            no_of_comp: "n*10 ==> 9999991 (n=10^6)",
            sync_a: "For all the nodes at the operating level, we need to solve that and then wait for other threads to complete it. Thus for the level we start we have to wait for all other threads to finish.",
            tcr_a_proc: "As number of processors increase, time decreases. This is because we get more number of disjoint nodes that can be computed independently.",
            mem_acc_l_and_s: "n*4 ==> 3999998 (n=10^6)",
            cache_coh_a: "There is no issue of problem here as the problem nodes in each thread are independent of each other. But there may arise one node of a thread in cache of another thread very frequently.",
            diff_faced: "Synchronization at the operating level  needs to be handled.",
            category_id: category1,
            mem_wall_a: "Memory access is one of the biggest issues in our approach, as for one iteration we have 3 memory access. Also they are of index i, 2*i, 2*i+1 which will be a major hurdle in the cache performance.",
            load_bal_a: "If the problem is not power of 2 and the number of processors are not power of 2 we are bound to have a code imbalance where different threads may have to do different amount of work."
        });

        Approach.create({
            approach_name: "Recursive Block Matrix Multiplication",
            user_id: [user1],
            descP: "As we have mentioned earlier there are (q*q) blocks.So every thread takes approximately equal (q*q)/p blocks. ",
            scr_a: "Speedup increases as problem size increases as suggested by Ahmdal's Effect",
            descS: "We divided input as well as output matrices into q*q blocks. Now after that matrix multiplication is done over this blocks. We have implemented function named local matrix multiplication which will multiply any two blocks that we have created. Hence after algorithm is over we get complete output. ",
            theo_speed: "p",
            granul_a: "Fine Granular because memory access is O(N^3) and also there are O(N^3) multiplications and O(N^3) additions. So computation/communication ratio is nearly constant.",
            user_id_1: "201401407",
            advantage: "Advantages of our approach is that each thread has own independent area of operation(There is no data dependency or function dependency). Disadvantage is that block size must divide problem size(N). (N%q==0)",
            tcr_a_problem: "Increases because time complexity is O(N^3)",
            complexityS: "Suppose input matrix has size of N rows and N columns then complexity is O(N^3).",
            complexityP: "O(N^3/p)",
            effcr_a: "Efficiency decreases as number of threads decreases",
            kfmc_a: "Karp flatt matric is almost constant in big problem sizes. Thus we can say that there is no parallel overhead bottleneck.",
            msapo: "Serial Overhead -> Taking input from file. Parallel overhead -> thread creation because I am creating threads in inner loop.",
            sched_rel_a: "We have used static scheduling because of cache efficiency. Because if any threads gets adjacent blocks, then some of the elements will already be present in cache, thus optimizing cache reuse.",
            _id: p2a4,
            costP: "O(N^3)",
            other_a: "Block size can play a huge role. Our implementation takes 1/3rd time than naive implementation. Cache sizes can also play a very huge role in determining block size. Because bigger the cache size , bigger blocks can fit into cache memory and thus reducing functional overhead of each block.",
            user_id_2: "201401448",
            tub: "p",
            problem_id: problem2,
            scalab_a: "Not very scalable because isoefficiency function is O(p^4)  and scalibility function is O(p^3).",
            estimated_ser_frac: "Negligible (O(1/N))",
            false_sharing_a: "False sharing can be a problem in matrix multiplication(block wise) but each thread has independent area of operation.",
            no_of_comp: "26398949376",
            sync_a: "Suppose matrices are divided into q*q blocks. Then we have to run outer loop q times. And we cannot run next iteration of outer loop without completing previous iteration.\nHowever no synchronization is needed if scheduling is fixed. ",
            tcr_a_proc: "Decreases because the amount of work for each thread decreases",
            mem_acc_l_and_s: "18555600896",
            cache_coh_a: "Cache coherence can be a problem in matrix multiplication(block wise) but each thread has independent area of operation.",
            diff_faced: "Scheduling was major issue. Wrong scheduling method can generate wrong answers. We have to use critical conditions if we do wrong scheduling .",
            category_id: category2,
            mem_wall_a: "Here computation is O(N^3) and memory access is also O(N^3). Thus as N increases,memory increases. Therefore memory bandwidth affects our performance. Also hardware implementation of cache can affect performance.",
            load_bal_a: "If problem size is not larger than load imbalance can occur. Some of the threads may get an extra block."
        });

        Approach.create({
            user_id: [user1],
            approach_name: "Reduction using data segmenting",
            descP: "We have given the approach of data-segmenting that's why we divided the array in p(number of processors) chunks and gave it to all the threads. They will sum it up individually using the serial implementation and after all the threads are done the master thread will combine them in global_sum.  The reason we are doing this using our own segmenting algorithm is that we should get more speedup if we divide the array in contiguous parts. We did not use array for storing local sum because we do not want any cache reading-writing problems i.e. Cache coherency.",
            scr_a: "We are getting super linear speedup for problem size of 10^7 and above because .. Each thread is loading data in its cache. During this time the latency is hidden.  ",
            descS: "We are given input from the file and we have to calculate the sum of all the elements in that array. For that we have implemented a for loop iterating and adding all the elements in an ans variable.",
            theo_speed: "serial/parallel = n/(n/p+p) = p/(1+(p*p)/n)    ",
            granul_a: "Data is divided in big chunks of size n/p, coarsely granular code. ",
            user_id_1: "201401222",
            advantage: "Our approach is best if p <<n. But as p increases this approach is not scaled enough. Because time complexity is O(n/p +p).",
            tcr_a_problem: "As problem size increases the time is increasing, but this increment is not linear.\nIt is increasing less. This is because for problem size of 10,100,1000,10000 most of the time is in synchronizing or launching the threads. But after 10^5 the trend is becoming smoother.",
            complexityS: "O(n)",
            complexityP: "work = O(n), steps = n/p+p ",
            effcr_a: "As speedup is super-linear the efficiency is also more than 1.for problem size more than or equal to 10^7. Now, the reason for this is same as the above.",
            kfmc_a: "Karp-Flatt metric is increasing as number of processors increases. But we are getting negative value for karp-flatt metric. The reason for this is that we are getting super linear speedup. e=(1/s-1/p)/(1-1/p) = (p-s)/(s*(p-1)) . denominator is +ve so p is less than s. For getting -ve karp-flatt metric.",
            msapo: "Most of the time (In serial and parallel) is wasted in taking the input from the file. But other than this there is no serial overhead. Surely parallel overhead is increasing as the number of processor increases  but it is negligible. \nIt is due to synchronization and thread spawning (Not significant (as we are getting super-linear speedup )).",
            sched_rel_a: "We implemented the code without parallel for directive. The data is simply segmented in p processors. The reason we are doing this using our own segmenting algorithm is that we should get more speedup if we divide the array in contiguous parts. We did not use array for storing local sum because we do not want any cache reading-writing problems i.e. Cache coherency.",
            _id: p1a3,
            costP: "cost = steps*num_threads=(n/p)*p+ p*1  //here we are joining the threads after their work and then the master thread is doing the work. so at that time we have to multiply it by p then 1.",
            other_a: "Nope",
            user_id_2: "201401409",
            tub: "1.04 speedup.  We are getting ~1.1 mainly because computational anomalies. It is coming down to ~1.0x for problem size of 10^7 and higher.",
            problem_id: problem1,
            scalab_a: "The problem is scalable in terms of increasing problem size. But it is not scalable in terms of increasing problem size. For p<<n we are getting the speedup. But if p become comparable to n then the scalability is not maintained. As we know there is serial fraction in our code which is joining p element's local sum. It will start to dominate when p is comparable.",
            estimated_ser_frac: "n operations. in serial part.    0.96% par is wasted in end to end part. taking input.",
            false_sharing_a: "There is no false sharing.  As we are not storing anything. No race conditions. The only store operations were in master thread. And at that time all other threads were joined.",
            no_of_comp: "(7p+n) operations in FLOPS (using the instructions.pdf) //only considered the algorithm part",
            sync_a: "All threads are doing same amount of work. That is why we should not get any synchronization related issues. And even if it happens it will happen only for very small number of iterations (1 or 2) (which is negligible). ",
            tcr_a_proc: "For parallel implementation the time is decreasing as number of processors increases. parallel for 1 processor is more than serial implementation. mainly because the time in launching the threads",
            mem_acc_l_and_s: "Store operations in the memory ==> (in parallel =p ,in serial=0) /* here we are cosidering only store operation in cache or memory storing in register is \"free\" */       Load operations in the memory ==> (in parallel =n ,in serial=n) /* here we are cosidering only load operation from cache or memory. loading from register is \"free\" */",
            cache_coh_a: "We are not storing after computing the sum in any global variable or array. We have implemented that part before but we are getting speedup of ~3 for 4 threads. In that implementation  cache coherence was the issue. // We have uploaded code which do not have this problem.",
            diff_faced: "The main difficulty was that we were getting speedup of around 3 for 4 threads. In the global array implementation. After that we considered this approach.",
            category_id: category1,
            mem_wall_a: "N/A. Even if we increase the speed of the processor there will be a limit due to memory.",
            load_bal_a: "Load is balanced. Because we are giving them the input manually. Final part(combining them) is only for p elements."
        });

        Approach.create({
            approach_name: "Reduction using Tree based approach",
            user_id: [user1],
            descP: "The input array is divided into sub-arrays and each sub-array is assigned to a thread. The thread computes the partial sum of its sub-array. To compute the final sum, we used the approach of a reduction tree. At each step, we compute the sum of the elements separated by the step size, and store the sum at the lower index of the two elements.. These sums can be computed parallely. The step size increases by a multiple of 2 with each iteration, to get a final single sum.",
            scr_a: "Speedup increases with increase in problem size and number of threads. For algorithm speedup, the 10**6 problem size is not able to fit in the L2 cache and hence has to go to the L3 cache. This significantly increases the data access time and hence speedup decreases. 4 threads gave a slightly different trend - in 10**6, the L2 cache is underutilized in the parallel code. For 10**6 to 10**7, in the serial code, the entire array is not able to fit in the L3 cache and has to access the RAM. However, in the parallel code even though RAM has to be accessed but since all the L1 and L2 cache are fully utilized a significant increased speedup is observed. From 10**7 to 10**8, the granularity of the problem also increases along with efficiency memory utilization. Hence, a super-linear speedup is observed.",
            descS: "It computes the sum of all the elements in the array to get a final sum.",
            theo_speed: "For n processors: O(n/logn), For p processors: O(n/(n/p + logp)))",
            granul_a: "This problem is coarsely granular. Since, computations are much higher than the required communication. We have used malloc() to allocate memory cation.",
            user_id_1: "201401433",
            advantage: "This approach makes the reduction algorithm cost efficient for p processors. However, though it is cost efficient, it is not work efficient as number of computations increases. In the task dependency tree, the threads are spawned in each iteration. Hence, the overheads increase.",
            tcr_a_problem: "As the problem size increases, the time taken increases since the number of computations increases.",
            complexityS: "O(n)",
            complexityP: "O(n/p + logp)",
            effcr_a: "Efficiency increases with increase in problem size, and decreases with increase in number of threads due to increase in overheads.  The anomalies in the efficiency curve is due to under-utilization in the cache which was also a result of decreased speedup in the speedup curves.",
            kfmc_a: "Due to superlinear speedup, the Karp-Flatt metric is negative. The anomaly for 4 threads and problem size 10**6 is due to the change in speedup as explained above. ",
            msapo: "We have used malloc() to allocate memory to array which an overhead present in both serial and parallel code. The parallel overhead in this approach is due to spawning the threads using Openmp.",
            sched_rel_a: "Since the workload for each thread is equally balanced while computing the partial sum, static scheduling is preferable. Later, the algorithm has task dependencies thus, dynamic scheduling would not be able to overcome these overheads.",
            _id: p1a1,
            costP: "O(n + plogp)",
            other_a: "Code Balance is 1 while computing partial sums, and changes to 2 while computing the reduction tree.",
            user_id_2: "201401439",
            tub: "Speedup is 0.298 using e2e and 20 by gprof.",
            problem_id: problem1,
            scalab_a: "Efficiency increases with increase in problem size, and decreases with increase in number of threads due to increase in overheads. Thus, it is weakly scalable.",
            estimated_ser_frac: "s = 0.971 by the e2e and alg method. Since we have included malloc() as well as file reading and writing in the e2e time, we get a very high serial fraction. Using gprof, s = 0.05",
            false_sharing_a: "Since each thread updates an element which is independent to the other threads, there is no false sharing.",
            no_of_comp: "n + p + logp",
            sync_a: "After each thread computes the partial sum, synchronization is required between the threads at each step (since it is a task dependency tree).",
            tcr_a_proc: "As the number of processors increases, the time taken for each problem decreases. This is because the time saved by the parallel computations is much higher than the parallel overhead.",
            mem_acc_l_and_s: "n + p + logp",
            cache_coh_a: "The array stored in the cache of different cores may overlap. Hence, updating an element in a particular cache may take more time in order to ensure coherency, resulting in a decreased speedup.",
            diff_faced: "N/A",
            category_id: category1,
            mem_wall_a: "The L3 cache can store 1.5 x 10 **7 integers. Thus, as the problem size increases beyond that, the input array would not fit into the cache and hence the RAM has to be accessed, so the memory access time increases.",
            load_bal_a: "The load balance for each thread is equally balanced while computing the partial sum. Later, since the number of computations becomes less than the number of threads, there is load imbalance."
        });

        Approach.create({
            approach_name: "Transpose based approach",
            user_id: [user1],
            descP: "We parallelize the outermost loop by using the \"pragma omp for\" directive. To solve the problems with race conditions, we have made the variables i,j,k private. We use default scheduling.",
            scr_a: "We don't get any speedup with increasing problem sizes or increasing number of cores. \nIdeally following should happen:\nThe speedup increases with increase in problem size for a given number of threads because time saved by parallel computation increases while the parallelization overhead remains the same. \nThe speedup also increases with the increase in number of threads. As we increase the number of cores, more work can be done simultaneously and so the speedup increases. ",
            descS: "In our serial code we multiply 2 nxn matrices using 3 nested for loops. We have written an optimised serial code where we take the transpose of the second matrix. ",
            theo_speed: "p",
            granul_a: "The code is finely granular. The code is divided into small chunks of data. ",
            user_id_1: "201401403",
            advantage: "Taking a transpose of the matrix greatly improves the serial time still this approach is not scalable. A block approach would be better for very high problem sizes.",
            tcr_a_problem: "As problem size increases, time take increases as there is are more computations to be performed.",
            complexityS: "O(n^3)",
            complexityP: "Work = n^3 Steps = (n^3)/p",
            effcr_a: "We couldn't achieve desirable or explainable speedup and thus we haven't analyzed the efficiency curve.",
            kfmc_a: "NA",
            msapo: "A major serial and parallel overhead is excessive memory access.",
            sched_rel_a: "Each iteration of the loop takes almost the same amount of time. Thus the default scheduling was good enough.",
            _id: p2a3,
            costP: "p*n^3/p = n^3",
            other_a: "Taking a transpose of the second matrix reduces memory access time because of reduced cache misses. This is because the system is row major and thus entire rows fit into the L1 cache.",
            user_id_2: "201401421",
            tub: "0.75*p",
            problem_id: problem2,
            scalab_a: "For very large problem sizes, a single row or column might not fit in the L1 cache and so further time will increase in memory access. ",
            estimated_ser_frac: "0.25",
            false_sharing_a: "We use default scheduling, so threads will be given rows that give adjacent elements in the result matrix. So adjacent elements will be updated by multiple threads at the same time and thus we do face a problem of false sharing.",
            no_of_comp: "6n^3 + 2n^2",
            sync_a: "NA",
            tcr_a_proc: "As number of processors increases, the time taken decreases. This is because more work is done simultaneously.",
            mem_acc_l_and_s: "3n^3+n^2",
            cache_coh_a: "There is no problem related to cache coherency.",
            diff_faced: "NA",
            category_id: category2,
            mem_wall_a: "Our memory to computation ratio is 1/2 which is very high. Thus our code is limited by the time taken to access memory.",
            load_bal_a: "Our work is well divided amongst the threads."
        });

        Approach.create({
            approach_name: "Middle Loop parallelization",
            user_id: [user1],
            descP: "The approach is implemented as follows:\nFor each row of the first matrix, a certain number of threads is created as per the input. The middle loop, i.e. the loop for columns of the second matrix (loop for j according to the formula mentioned in the previous question) is parallelized, which means that for each row of matrix A, different threads traverse different columns of the matrix B. The sub tasks here are the column iterations. The division is done by static scheduling with a chunk size of 4.",
            scr_a: "Speedup in general increases as the number of processors increases. This is due to more load distribution for same problem size, resulting in a decline in the time required for parallel version. The speedup also increases as the problem size increases. This is because load distribution starts dominating more and more over the parallel overhead. The maximum value obtained is 3.5 for 4 cores, which is less than 4 due to synchronization overheads and poor cache utilization. Cache utilization is almost negligible because the loop is column oriented which creates no difference in the cache for the next value to be used, so all accesses are memory accesses. Cache reusability is negligible. There are some anomalies where for small problem size the speedup is more for 2 cores than that of 4 cores, which is due to parallel overhead dominating for small problem sizes. Eventually, load distribution starts dominating and the speedup increases.",
            descS: "In the serial implementation, there are three loops. To calculate the product matrix, Product[i][j] = Sum( A[i][k] + B[k][j] ) is used. The outer loop traverses the rows of the first matrix (loop for i), the middle loop traverses the columns of the second matrix (loop for j) and the inner loop calculates the sum of products for that particular row and column (loop for k). In this way, the product of the two input matrices (with values in double) is calculated.",
            theo_speed: "Theoretical speedup=number of processors=p",
            granul_a: "Granularity increases as the problem size increases because communication between the threads does not change but the computation per thread increases. So the problem is finely granular for small problem sizes but coarsely granular for large problem sizes.",
            user_id_1: "201401098",
            advantage: "Disadvantage of the approach is that it parallelizes the access to columns of the second matrix. But since C is a row major language, so once a column is accessed, the next elements do not get stored in cache. So each and every time, the elements will need to be accessed from the main memory. So cache utilization is very poor in this approach, and the memory latency is high. The machine balance is poor due to these reasons.\nPeak performance of our algorithm (FLOPS*clock freq*number of cores) is significantly higher than the serial algorithm. Peak performance for 4 cores is 54.9 MFLOPS/sec.",
            tcr_a_problem: "Time taken increases as the problem size increases, as the complexity is O(n^3) for both serial and parallel. So the curve shows an asymptotically increasing trend due to more memory accesses and more computations.",
            complexityS: "O(n^3)",
            complexityP: "Work complexity: O(n^3). It will not depend on the number of processors in this case because n>>p. Also, if synchronization overhead is included, still n^3 would dominate over the overhead inside the O notation. Step complexity is not applicable here because every element of the product needs to be calculated independently of the others, so there is no spanning or reduction involved.",
            effcr_a: "Efficiency increases with problem size and reaches a maximum of about 0.9 for 3 cores, 0.7 for 2 and 0.8 for 4 cores. In general, the efficiency is less for more number of cores. This is because the speedup doesn't increase as much as it should increase ideally for more number of processors, due to synchronization overheads and parallel overheads caused by repeated forking and joining of threads for each row of the first matrix.",
            kfmc_a: "The Karp-Flatt metric (i.e. the serial fraction of the code) decreases as the problem size increases because the middle loop parallelization is for a higher number of operations as the problem size increases. So the parallel fraction would increase.\nIt also decreases as the number of processors increases. This is because with more number of processors, parallelization increases, thereby decreasing the serial fraction of the code.",
            msapo: "Parallel overhead: The parallel algorithm repeatedly creates and synchronizes threads for each row of the first matrix. This adds a very significant synchronization overhead.",
            sched_rel_a: "Scheduling statically with a chunk size of 4 gave us a higher speedup than a chunk size of 1 and a size of 8. Here 4 comes out to be the optimal value of the number of iterations that should be assigned to a particular thread, to achieve the maximum speedup.",
            _id: p2a1,
            costP: "p*n^3; where p is the number of processors and n is the problem size.",
            other_a: "Cache utilization and spatial locality is not used well enough in this approach because of the row major nature of C and the column division approach of this code. It would have gained a better speedup if the rows were divided, i.e. the outer loop was parallelized.",
            user_id_2: "201401428",
            tub: "Upper bound based on Amdahl's Law for speedup is: (p/0.096p+0.904), which is nearly 3.2 for end to end. The speedup achieved for the algorithm will be a little more than this, since overheads due to timer calculations, printing, file read/write etc will be decreased.",
            problem_id: problem2,
            scalab_a: "The problem in general is scalable because the speedup increases with problem size as well as with the number of processors.",
            estimated_ser_frac: "Using (e2e-alg)/e2e, we get the serial fraction as approximately 0.096.",
            false_sharing_a: "False sharing is not applicable here because of the reason mentioned above.",
            no_of_comp: "17.179869184 GFLOPS for problem size of 2048.",
            sync_a: "Synchronization needs to be done repeatedly after every row of the first matrix. This adds a significant synchronization overhead in the results.",
            tcr_a_proc: "The time taken decreases as the number of processors increases for the same problem size. This is because of efficient load distribution between the threads. The same work is now distributed among more threads, so time taken is less.",
            mem_acc_l_and_s: "Load: 2*(n^3), Store: n^2; Total: 2*n^3 + n^2.",
            cache_coh_a: "Cache coherence doesn't apply here since different elements of the product matrix are updated by threads and there is no case where a same variable is updated by different threads.",
            diff_faced: "The iterating variable in the inner loop needs to be made private for each thread while parallelizing the middle loop. Otherwise, race conditions may occur and the variable may attain a higher value than its upper limit, which would cause a segmentation fault while accessing the memory using that variable. Also, the variable that calculates the sum of products for each row and column needs to be private so that its value isn't altered by some other thread.",
            category_id: category2,
            mem_wall_a: "Memory wall creates a delay in load and store operations which makes memory access slower than the computations done. And also since the cache is not utilized well, so memory access to be done is more.",
            load_bal_a: "Load balance is done well for all sizes because of static scheduling with chunk size 4. This improves the speedup, since no thread does more work than the other."
        });
    });

